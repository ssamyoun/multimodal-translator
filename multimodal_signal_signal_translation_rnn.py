# -*- coding: utf-8 -*-
"""Signal-Signal Translation RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AxZROnKxjQ8y2zEEaK1xDfgcfxjZKEkz
"""

from os import listdir
from numpy import asarray
from numpy import vstack
from numpy import savez_compressed
from numpy import load
from numpy import zeros
from numpy import ones
from numpy.random import randint
from keras.optimizers import Adam
from keras.initializers import RandomNormal
from keras.models import Model
from keras.models import Input
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Activation
from keras.layers import Concatenate
from keras.layers import Dropout
from keras.layers import BatchNormalization
from keras.layers import LeakyReLU
from matplotlib import pyplot
import random
import matplotlib.pyplot as plt
import os, time,itertools, pickle
import tensorflow as tf
import numpy as np
import os
from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Sequential
from keras.layers import Dense
from keras import optimizers
import numpy as np
import os
import sys
import gc
import pickle
from sklearn.model_selection import train_test_split
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Input
from keras.models import Sequential, model_from_json, load_model
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy as np
import sys, os
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Bidirectional
from keras.layers import LSTM
from keras.models import Model
from numpy import array
from numpy import mean
from numpy import var
import pandas as pd
from tqdm import tqdm
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

import pandas as pd
from scipy import stats
import scipy.signal as scisig
import scipy.stats
np.set_printoptions(suppress=True)

#Google Drive Authetication and setting data path
from google.colab import drive
drive.mount('/content/drive')
DATA_PATH = "/content/drive/My Drive/Colab Notebooks/Wrist-Chest/data/"

#Pix2Pix GAN functions
def get_discriminator(data_shape):
	input_tensor = Input(shape=data_shape)
	output_tensor = Input(shape=data_shape)
	merged = Concatenate()([input_tensor, output_tensor])
	#print("get_discriminator: merged shape:", merged.shape)
	# C64
	d = Conv2D(64, kr_size, strides=st_count, padding='same', kernel_initializer=weight_init)(merged)
	d = LeakyReLU(alpha=0.2)(d)
	# C128
	d = Conv2D(128, kr_size, strides=st_count, padding='same', kernel_initializer=weight_init)(d)
	d = BatchNormalization()(d)
	d = LeakyReLU(alpha=0.2)(d)
	# C256
	d = Conv2D(256, kr_size, strides=st_count, padding='same', kernel_initializer=weight_init)(d)
	d = BatchNormalization()(d)
	d = LeakyReLU(alpha=0.2)(d)
	# C512
	d = Conv2D(512, kr_size, strides=st_count, padding='same', kernel_initializer=weight_init)(d)
	d = BatchNormalization()(d)
	d = LeakyReLU(alpha=0.2)(d)
	# second last output layer
	d = Conv2D(512, kr_size, padding='same', kernel_initializer=weight_init)(d)
	d = BatchNormalization()(d)
	d = LeakyReLU(alpha=0.2)(d)
	# final output
	d = Conv2D(1, kr_size, padding='same', kernel_initializer=weight_init)(d)
	final_out = Activation('sigmoid')(d)
	# get model
	model = Model([input_tensor, output_tensor], final_out)
	# compile model
	opt = Adam(lr=0.0002, beta_1=0.5)
	model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])
	return model

def get_generator(data_shape):
	input_tensor = Input(shape=data_shape)
	# encoder
	e1 = encoder_block(input_tensor, 64, batchnorm=False)
	e2 = encoder_block(e1, 128)
	e3 = encoder_block(e2, 256)
	b = Conv2D(256, kr_size, strides=st_count, padding='same', kernel_initializer=weight_init)(e3)
	b = Activation('relu')(b)
	# decoder
	#print("encoded_blocks shape:", b.shape, e3.shape)
	d1 = decoder_block(b, e3, 256)
	d2 = decoder_block(d1, e2, 128)
	d3 = decoder_block(d2, e1, 64)
	#print("decoded_blocks shape:", b.shape, e3.shape)
	# output
	g = Conv2DTranspose(1, kr_size, strides=st_count, padding='same', kernel_initializer=weight_init)(d3)
	out_data = Activation('tanh')(g)
	# define model
	model = Model(input_tensor, out_data)
	return model

def encoder_block(layer_in, n_filters, batchnorm=True):
	# add downsampling layer
	g = Conv2D(n_filters, kr_size, strides=st_count, padding='same', kernel_initializer=weight_init)(layer_in)
	# conditionally add batch normalization
	if batchnorm:
		g = BatchNormalization()(g, training=True)
	# leaky relu activation
	g = LeakyReLU(alpha=0.2)(g)
	return g

# define a decoder block
def decoder_block(layer_in, skip_in, n_filters, dropout=True):
	# weight initialization
	init = RandomNormal(stddev=0.02)
	# add upsampling layer
	g = Conv2DTranspose(n_filters, kr_size, strides=st_count, padding='same', kernel_initializer=init)(layer_in)
	# add batch normalization
	g = BatchNormalization()(g, training=True)
	# conditionally add dropout
	if dropout:
		g = Dropout(0.5)(g, training=True)
	# merge with skip connection
	g = Concatenate()([g, skip_in])
	# relu activation
	g = Activation('relu')(g)
	return g

def get_gan(g_model, d_model, data_shape):
	# make weights in the discriminator not trainable
	d_model.trainable = False
	# define the source data
	input_tensor = Input(shape=data_shape)
	# connect the source data to the generator input
	gen_out = g_model(input_tensor)
	# connect the source input and generator output to the discriminator input
	dis_out = d_model([input_tensor, gen_out])
	# src data as input, generated data and classification output
	model = Model(input_tensor, [dis_out, gen_out])
	# compile model
	opt = Adam(lr=0.0002, beta_1=0.5)
	model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])
	return model
# GAN Training
def get_real_samples(train_X, train_Y, n_samples, patch_shape):
  # choose random instances
	ix = randint(0, train_X.shape[0], n_samples)
	# retrieve selected datas
	X1, X2 = train_X[ix], train_Y[ix]
	# generate 'real' class labels (1)
	y = ones((n_samples, patch_shape, patch_shape, 1))
	return [X1, X2], y

# generate a batch of datas, returns datas and targets
def predict_fake_samples(g_model, samples, patch_shape):
	# generate fake instance
	X = g_model.predict(samples)
	# create 'fake' class labels (0)
	y = zeros((len(X), patch_shape, patch_shape, 1))
	return X, y

def train_gan(d_model, g_model, gan_model, train_X, train_Y, n_epochs=100, n_batch=1):
	# calculate #of batches per training epoch, #of training iterations
	n_patch = d_model.output_shape[1]
	bat_per_epo = int(len(train_X) / n_batch)
	n_steps = bat_per_epo * n_epochs
	# manually enumerate epochs
	for i in range(n_steps):
		# select a batch of real samples
		[X_real_wrist, X_real_chest], y_real = get_real_samples(train_X, train_Y, n_batch, n_patch)
		# generate a batch of fake samples
		X_fake_chest, y_fake = predict_fake_samples(g_model, X_real_wrist, n_patch)
		# update discriminator for real samples
		d_loss1 = d_model.train_on_batch([X_real_wrist, X_real_chest], y_real)
		# update discriminator for generated samples
		d_loss2 = d_model.train_on_batch([X_real_wrist, X_fake_chest], y_fake)
		# update the generator
		g_loss, _, _ = gan_model.train_on_batch(X_real_wrist, [y_real, X_real_chest])
		# summarize performance
		print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))
		# summarize model performance
		if g_loss < ls_min:
				break

def train_model(train_X_GAN,train_Y_GAN):
  #print("train_X", len(train_X), train_X.shape)
  #print("train_X_GAN", len(train_X_GAN), train_X_GAN.shape)
  data_shape = train_X_GAN.shape[1:]
  print("data_shape", data_shape)
  discriminator_model = get_discriminator(data_shape)
  generator_model = get_generator(data_shape)
  gan_model = get_gan(generator_model, discriminator_model, data_shape)
  # train gan
  train_gan(discriminator_model, generator_model, gan_model, train_X_GAN, train_Y_GAN)
  return generator_model

#Read and Process input data
def read_file(folder_name, file_name):
    new_path = folder_name
    with open(os.path.expanduser(new_path+'/'+file_name), 'rb') as file:
        data = pickle.load(file, encoding='latin1')
    return data

def get_subject_EDA_data(subjectId):
    S_DATA_PATH = DATA_PATH #+ subjectId
    s_data = read_file(S_DATA_PATH, subjectId + '.pkl')
    s_chestEDA_arr = s_data["signal"]["chest"]["EDA"]
    s_wristEDA_list = s_data["signal"]["wrist"]["EDA"]
    s_chestEDA_list = []
    s_chestEDA_list = s_chestEDA_arr[::175,:]
    return (s_wristEDA_list, s_chestEDA_list)


def get_list_data(input_data):
    output_data = []
    for i in range(0, len(input_data)-window_length, window_length):
      output_data.append(input_data[i:i+window_length])
    return output_data

def reshape_for_gan(input_data):
  input_data = np.array(input_data)
  output_data = np.reshape(input_data, input_data.shape + (1,))
  return output_data

def reshaped_data_for_gan(input_data):  #windowed and then 3D
	output_data = []
	for i in range(0, len(input_data)-window_length, window_length):
		output_data.append(input_data[i:i+window_length])
	output_data = np.array(output_data)
	output_data = np.reshape(output_data, output_data.shape + (1,))
	return output_data

def reshape_for_linear_regression(input_data):
    output_data = []
    for i in input_data:
        for j in i:
            output_data.append(j)
    return output_data

def reshape_for_lstm(input_data):
    output_data = []
    for i in range(0, len(input_data)-window_length, 6):
      output_data.append(input_data[i:i+window_length])
    output_data = np.array(output_data)
    return output_data

def data_with_selected_labels(wd,label_data):
  #wrist
  eda_list= []
  ieda,ilbl = 0,0
  seda = len(wd)
  beda,blbl= freq_dict['EDA'], freq_dict['label']
  while (ieda < seda) :
    l = label_data[ilbl]
    if l>0 and l<4 :
      for i in range(ieda, ieda+beda):
        eda_list.append(wd[i])
    ieda += beda
    ilbl += blbl

  wd = np.asarray(eda_list)
  return (wd)

def wstats(df, i, blocksize):
  temp = df[i:i+blocksize].to_numpy().flatten()
  return [np.mean(temp),np.std(temp),np.amin(temp),np.amax(temp)]

def wstatsnp(arr, si, ei):
  temp = arr[si:ei]
  a = [np.mean(temp),np.std(temp),np.amin(temp),np.amax(temp)]
  return a

def get_data(subject):
  with open(DATA_PATH + 'S{}.pkl'.format(subject), 'rb') as file:
        data = pickle.load(file, encoding='latin1')
        return (data["signal"]["wrist"], data["signal"]["chest"], data["label"])

def get_data1(subject):
  with open(SAVE_DATA_PATH2 + 't_S{}.pkl'.format(subject), 'rb') as file:
        data = pickle.load(file, encoding='latin1')
        return (data)

def get_data2(subject):
  with open(SAVE_DATA_PATH2 + 'tECG_S{}.pkl'.format(subject), 'rb') as file:
        data = pickle.load(file, encoding='latin1')
        return (data)

def convert_data_for_sequential_model(input_data):
    output_data = []
    for i in range(0, len(input_data)-24, 6):
      output_data.append(input_data[i:i+24])
    output_data = np.array(output_data)
    return output_data

window_length = 16

DATA_PATH = "/content/drive/My Drive/Colab Notebooks/WESAD_processed data/"
all_subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]
A_data = {}
B_data = {}

for subject in all_subject_ids:
  with open(f'{DATA_PATH}S{subject}.pkl', 'rb') as file:
      data1 = pickle.load(file, encoding='latin1')
      wrist_eda = data1['signal']['wrist']['BVP'][::16,:]
      chest_eda = data1['signal']['chest']['Resp'][::175,:] #ECG Resp EDA
      A_data[subject] = wrist_eda
      B_data[subject] = chest_eda
      #print(wrist_eda.shape, chest_eda.shape)

ls_min = 15.00 #EDA10 ECG20/15 #RESP25
print("ECG")

weight_init = RandomNormal(stddev=0.02)
st_count = (2,1)
kr_size = (4,1)
mae_norm = []
mae_unnorm = []

for subj in all_subject_ids:
  x_train, y_train, x_test, y_test = [],[],A_data[subj],B_data[subj]
  for sid in all_subject_ids:
    if sid != subj:
      x_train.extend(A_data[sid])
      y_train.extend(B_data[sid])

  mu_x = np.mean(x_train, axis=0)
  sigma_x = np.std(x_train, axis=0)
  x_train_norm = (x_train-mu_x)/sigma_x
  x_test_norm = (x_test-mu_x)/sigma_x

  mu_y = np.mean(y_train, axis=0)
  sigma_y = np.std(y_train, axis=0)
  y_train_norm = (y_train-mu_y)/sigma_y
  y_test_norm = (y_test-mu_y)/sigma_y

  ###

  train_X_DL, train_Y_DL, test_X_DL, test_Y_DL = reshape_for_lstm(x_train_norm), reshape_for_lstm(y_train_norm), reshape_for_lstm(x_test_norm), reshape_for_lstm(y_test_norm)
  biLSTM_model = Sequential()
  biLSTM_model.add(Bidirectional(LSTM(32, return_sequences=True, stateful=False), input_shape=(window_length, 1)))
  biLSTM_model.add(Dense(1, activation='relu'))
  biLSTM_model.compile(loss='mean_squared_error', optimizer='rmsprop')
  biLSTM_model.fit(train_X_DL, train_Y_DL, batch_size=32, epochs=10)
  print("LSTM")

  test_Y_DL_pred = biLSTM_model.predict(test_X_DL)
  a_norm = test_Y_DL.reshape((-1, ))
  b_norm = test_Y_DL_pred.reshape((-1, ))

  ###

  '''train_X_GAN_norm, train_Y_GAN_norm = reshaped_data_for_gan(x_train_norm), reshaped_data_for_gan(y_train_norm)
  test_X_GAN_norm, test_Y_GAN_norm = reshaped_data_for_gan(x_test_norm), reshaped_data_for_gan(y_test_norm)

  model = train_model(train_X_GAN_norm, train_Y_GAN_norm)
  test_Y_GAN_pred_norm = model.predict(test_X_GAN_norm)

  a_norm = test_Y_GAN_norm.reshape((-1, ))
  b_norm = test_Y_GAN_pred_norm.reshape((-1, ))'''

  ###

  a = a_norm*sigma_x + mu_x
  b = b_norm*sigma_y + mu_y
  print("Subject", subj) #

  norm_err= mean_absolute_error(a_norm, b_norm)
  unnorm_err= mean_absolute_error(a, b)

  mae_norm.append(norm_err)
  mae_unnorm.append(unnorm_err)

  print("\n  NORMALIZED:", norm_err)
  print("\n  UNNORMAILZED- MAE:", unnorm_err)

print(mae_norm)
print(mae_unnorm)

print(np.mean(mae_norm), np.std(mae_norm))
print("------")
print(np.mean(mae_unnorm), np.std(mae_unnorm))